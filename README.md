# Airflow

ğŸ”¹ Overview
This project demonstrates a multi-pipeline workflow orchestrated using Apache Airflow, showcasing how multiple independent data pipelines can run within a single DAG.
It includes end-to-end processes such as ETL (Extractâ€“Transformâ€“Load), API data processing, and data quality validation, using Python functions, XComs, and Airflow task orchestration.
The workflows simulate real-world data engineering scenarios by coordinating ingestion, transformation, and validation tasks in a modular and scalable design.

![image alt]()

ğŸ“ Pipeline Overview
1ï¸âƒ£ ETL Pipeline (Extract â†’ Transform â†’ Load)
Extract sample dataset
Transform data by adding computed columns
Load (print/store) the processed output
2ï¸âƒ£ API Data Pipeline
Simulate external API call
Process JSON response
Store the cleaned and structured output
3ï¸âƒ£ Data Quality Pipeline
Perform null-value checks
Run duplicate detection tasks

ğŸ”§ Tools & Technologies Used
â¤ Apache Airflow (workflow orchestration)
â¤ Python (Pandas, JSON handling)
â¤ Docker / Local Airflow Setup
â¤ Airflow UI (for triggering and monitoring pipelines)

